# -*- coding: utf-8 -*-
"""DataSoc_2025_Interuni_Workshop_XGBoost

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lLlhXF4yN-r1GhQb2JTnY7bPR2_6Vy12
"""

# XGBoost Time Series Forecasting Demo
# Using the classic AirPassengers dataset

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error

"""# 1. Load Dataset"""

url = "https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv"
data = pd.read_csv(url, parse_dates=['Month'], index_col='Month')

print(data.head())

# Plot raw data
data.plot(figsize=(10,5), title="Monthly Airline Passengers")
plt.show()

"""# 2. Feature Engineering"""

# Function to create lag features
def create_lagged_features(series, n_lags=12):
    df = pd.DataFrame(series.copy())
    for lag in range(1, n_lags+1):
        df[f'lag_{lag}'] = df['Passengers'].shift(lag)
    df['month'] = df.index.month
    return df.dropna()

n_lags = 12
df = create_lagged_features(data['Passengers'], n_lags)

print(df.head())

"""# 3. Train/Test Split (time based)"""

train_size = int(len(df) * 0.8)
train, test = df.iloc[:train_size], df.iloc[train_size:]

X_train, y_train = train.drop('Passengers', axis=1), train['Passengers']
X_test, y_test = test.drop('Passengers', axis=1), test['Passengers']

"""# 4. Train XGBoost Model"""

# ---------------------------------------------------
# 4. Train XGBoost Model
# ---------------------------------------------------
model = XGBRegressor(objective='reg:squarederror', n_estimators=200)
model.fit(X_train, y_train)

"""# 5. Make Predictions"""

y_pred = model.predict(X_test)

rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f"Test RMSE: {rmse:.2f}")

# Plot predictions vs actual
plt.figure(figsize=(10,5))
plt.plot(y_test.index, y_test, label="Actual")
plt.plot(y_test.index, y_pred, label="Predicted")
plt.legend()
plt.title("XGBoost Forecasting - Air Passengers")
plt.show()

"""# 6. Discussions:
- XGBoost uses lagged features to learn patterns.
- This is not a true sequential forecast, but works well for short-horizon prediction.
- For long forecasts, iterative prediction (feeding back predicted values) is required.
"""